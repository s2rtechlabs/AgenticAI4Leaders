{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4: RAG (Retrieval-Augmented Generation) - Hands-On Exercise\n",
    "\n",
    "## Learning Objectives\n",
    "1. Understand how embeddings represent text semantically\n",
    "2. Learn vector similarity search\n",
    "3. Build a simple RAG system using ChromaDB\n",
    "4. See RAG in action with real queries\n",
    "\n",
    "## What We'll Build\n",
    "A knowledge base about AI/ML concepts that can answer questions using RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install chromadb sentence-transformers openai python-dotenv scikit-learn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports successful!\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Understanding Embeddings\n",
    "\n",
    "Embeddings convert text into numerical vectors that capture semantic meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 4\n",
      "Embedding dimension: 384\n",
      "\n",
      "First embedding (truncated): [ 0.13489066 -0.03206333 -0.02033523  0.03590099 -0.0283331   0.04150213\n",
      "  0.03315875  0.03660566  0.00861661  0.03763952]...\n"
     ]
    }
   ],
   "source": [
    "# Initialize embedding model\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Example sentences\n",
    "sentences = [\n",
    "    \"The cat sits on the mat\",\n",
    "    \"A feline rests on the carpet\",\n",
    "    \"The dog runs in the park\",\n",
    "    \"Python is a programming language\"\n",
    "]\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = embedding_model.encode(sentences)\n",
    "\n",
    "print(f\"Number of sentences: {len(sentences)}\")\n",
    "print(f\"Embedding dimension: {embeddings.shape[1]}\")\n",
    "print(f\"\\nFirst embedding (truncated): {embeddings[0][:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Compute Similarity\n",
    "\n",
    "Calculate cosine similarity between embeddings to see which sentences are semantically similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Matrix:\n",
      "============================================================\n",
      "\n",
      "The cat sits on the mat\n",
      "  vs 'A feline rests on the carpet': 0.5612\n",
      "  vs 'The dog runs in the park': 0.0949\n",
      "  vs 'Python is a programming language': 0.0317\n",
      "\n",
      "A feline rests on the carpet\n",
      "  vs 'The cat sits on the mat': 0.5612\n",
      "  vs 'The dog runs in the park': 0.1561\n",
      "  vs 'Python is a programming language': 0.1009\n",
      "\n",
      "The dog runs in the park\n",
      "  vs 'The cat sits on the mat': 0.0949\n",
      "  vs 'A feline rests on the carpet': 0.1561\n",
      "  vs 'Python is a programming language': 0.0462\n",
      "\n",
      "Python is a programming language\n",
      "  vs 'The cat sits on the mat': 0.0317\n",
      "  vs 'A feline rests on the carpet': 0.1009\n",
      "  vs 'The dog runs in the park': 0.0462\n",
      "\n",
      "üí° Notice: Sentences 0 and 1 have high similarity despite different words!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compute similarity matrix\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "print(\"Similarity Matrix:\")\n",
    "print(\"=\" * 60)\n",
    "for i, sent1 in enumerate(sentences):\n",
    "    print(f\"\\n{sent1}\")\n",
    "    for j, sent2 in enumerate(sentences):\n",
    "        if i != j:\n",
    "            print(f\"  vs '{sent2}': {similarity_matrix[i][j]:.4f}\")\n",
    "\n",
    "print(\"\\nüí° Notice: Sentences 0 and 1 have high similarity despite different words!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Setting Up ChromaDB Vector Database\n",
    "\n",
    "ChromaDB is an in-memory vector database perfect for RAG applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created collection: ai_knowledge_base\n",
      "   Collection count: 0\n"
     ]
    }
   ],
   "source": [
    "# Initialize ChromaDB client (in-memory)\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "# Create embedding function\n",
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Create or get collection\n",
    "collection = chroma_client.create_collection(\n",
    "    name=\"ai_knowledge_base\",\n",
    "    embedding_function=sentence_transformer_ef,\n",
    "    metadata={\"description\": \"AI/ML concepts knowledge base\"}\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Created collection: {collection.name}\")\n",
    "print(f\"   Collection count: {collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Populating the Knowledge Base\n",
    "\n",
    "Let's add documents about AI/ML concepts to our vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Added 10 documents to the knowledge base\n",
      "   Total documents in collection: 10\n"
     ]
    }
   ],
   "source": [
    "# Knowledge base documents\n",
    "documents = [\n",
    "    {\n",
    "        \"id\": \"doc1\",\n",
    "        \"text\": \"Machine Learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. It focuses on developing algorithms that can access data and use it to learn for themselves.\",\n",
    "        \"metadata\": {\"category\": \"ML Basics\", \"topic\": \"Introduction\"}\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc2\",\n",
    "        \"text\": \"Deep Learning is a subset of machine learning that uses neural networks with multiple layers. These deep neural networks can learn hierarchical representations of data, making them particularly effective for tasks like image recognition and natural language processing.\",\n",
    "        \"metadata\": {\"category\": \"Deep Learning\", \"topic\": \"Neural Networks\"}\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc3\",\n",
    "        \"text\": \"Natural Language Processing (NLP) is a branch of AI that helps computers understand, interpret, and manipulate human language. NLP combines computational linguistics with machine learning and deep learning models.\",\n",
    "        \"metadata\": {\"category\": \"NLP\", \"topic\": \"Language Understanding\"}\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc4\",\n",
    "        \"text\": \"Transformers are a type of neural network architecture that has revolutionized NLP. They use self-attention mechanisms to process sequential data in parallel, making them more efficient than previous architectures like RNNs. GPT and BERT are examples of transformer models.\",\n",
    "        \"metadata\": {\"category\": \"Deep Learning\", \"topic\": \"Transformers\"}\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc5\",\n",
    "        \"text\": \"Embeddings are dense vector representations of data that capture semantic meaning. In NLP, word embeddings represent words as vectors in a continuous vector space where semantically similar words are closer together.\",\n",
    "        \"metadata\": {\"category\": \"NLP\", \"topic\": \"Embeddings\"}\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc6\",\n",
    "        \"text\": \"RAG (Retrieval-Augmented Generation) combines information retrieval with text generation. It retrieves relevant documents from a knowledge base and uses them as context for generating accurate, grounded responses. This approach reduces hallucinations in LLMs.\",\n",
    "        \"metadata\": {\"category\": \"Advanced AI\", \"topic\": \"RAG\"}\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc7\",\n",
    "        \"text\": \"Vector databases store and index high-dimensional vectors for efficient similarity search. They use algorithms like HNSW (Hierarchical Navigable Small World) or IVF (Inverted File Index) to quickly find similar vectors, which is essential for RAG systems.\",\n",
    "        \"metadata\": {\"category\": \"Advanced AI\", \"topic\": \"Vector Databases\"}\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc8\",\n",
    "        \"text\": \"Fine-tuning is the process of taking a pre-trained model and training it further on a specific task or domain. This allows the model to adapt to specialized use cases while leveraging the knowledge learned during pre-training.\",\n",
    "        \"metadata\": {\"category\": \"ML Basics\", \"topic\": \"Model Training\"}\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc9\",\n",
    "        \"text\": \"Prompt Engineering is the practice of designing effective prompts to get desired outputs from language models. It involves techniques like few-shot learning, chain-of-thought prompting, and role-based prompting.\",\n",
    "        \"metadata\": {\"category\": \"LLM Usage\", \"topic\": \"Prompting\"}\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc10\",\n",
    "        \"text\": \"Agentic AI refers to AI systems that can autonomously plan, execute tasks, and make decisions to achieve goals. These agents can use tools, interact with environments, and adapt their strategies based on feedback.\",\n",
    "        \"metadata\": {\"category\": \"Advanced AI\", \"topic\": \"AI Agents\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "# Add documents to collection\n",
    "collection.add(\n",
    "    documents=[doc[\"text\"] for doc in documents],\n",
    "    metadatas=[doc[\"metadata\"] for doc in documents],\n",
    "    ids=[doc[\"id\"] for doc in documents]\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Added {len(documents)} documents to the knowledge base\")\n",
    "print(f\"   Total documents in collection: {collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Semantic Search with Vector Database\n",
    "\n",
    "Now let's query our knowledge base using semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Query: What are neural networks?\n",
      "======================================================================\n",
      "\n",
      "Result 1 (Distance: 0.3759)\n",
      "Category: Deep Learning | Topic: Neural Networks\n",
      "Text: Deep Learning is a subset of machine learning that uses neural networks with multiple layers. These deep neural networks can learn hierarchical repres...\n",
      "\n",
      "Result 2 (Distance: 0.5338)\n",
      "Category: Deep Learning | Topic: Transformers\n",
      "Text: Transformers are a type of neural network architecture that has revolutionized NLP. They use self-attention mechanisms to process sequential data in p...\n",
      "\n",
      "Result 3 (Distance: 0.5632)\n",
      "Category: ML Basics | Topic: Introduction\n",
      "Text: Machine Learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed....\n",
      "\n",
      "======================================================================\n",
      "Query: How do I represent words as numbers?\n",
      "======================================================================\n",
      "\n",
      "Result 1 (Distance: 0.6329)\n",
      "Category: NLP | Topic: Embeddings\n",
      "Text: Embeddings are dense vector representations of data that capture semantic meaning. In NLP, word embeddings represent words as vectors in a continuous ...\n",
      "\n",
      "Result 2 (Distance: 0.7486)\n",
      "Category: NLP | Topic: Language Understanding\n",
      "Text: Natural Language Processing (NLP) is a branch of AI that helps computers understand, interpret, and manipulate human language. NLP combines computatio...\n",
      "\n",
      "Result 3 (Distance: 0.8036)\n",
      "Category: Deep Learning | Topic: Transformers\n",
      "Text: Transformers are a type of neural network architecture that has revolutionized NLP. They use self-attention mechanisms to process sequential data in p...\n",
      "\n",
      "======================================================================\n",
      "Query: What is the best way to reduce AI hallucinations?\n",
      "======================================================================\n",
      "\n",
      "Result 1 (Distance: 0.7133)\n",
      "Category: Advanced AI | Topic: RAG\n",
      "Text: RAG (Retrieval-Augmented Generation) combines information retrieval with text generation. It retrieves relevant documents from a knowledge base and us...\n",
      "\n",
      "Result 2 (Distance: 0.7859)\n",
      "Category: Advanced AI | Topic: AI Agents\n",
      "Text: Agentic AI refers to AI systems that can autonomously plan, execute tasks, and make decisions to achieve goals. These agents can use tools, interact w...\n",
      "\n",
      "Result 3 (Distance: 0.8530)\n",
      "Category: NLP | Topic: Language Understanding\n",
      "Text: Natural Language Processing (NLP) is a branch of AI that helps computers understand, interpret, and manipulate human language. NLP combines computatio...\n"
     ]
    }
   ],
   "source": [
    "def search_knowledge_base(query: str, n_results: int = 3):\n",
    "    \"\"\"\n",
    "    Search the knowledge base for relevant documents.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query\n",
    "        n_results: Number of results to return\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing results\n",
    "    \"\"\"\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=n_results\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test queries\n",
    "test_queries = [\n",
    "    \"What are neural networks?\",\n",
    "    \"How do I represent words as numbers?\",\n",
    "    \"What is the best way to reduce AI hallucinations?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    results = search_knowledge_base(query, n_results=3)\n",
    "    \n",
    "    for i, (doc, metadata, distance) in enumerate(zip(\n",
    "        results['documents'][0],\n",
    "        results['metadatas'][0],\n",
    "        results['distances'][0]\n",
    "    )):\n",
    "        print(f\"\\nResult {i+1} (Distance: {distance:.4f})\")\n",
    "        print(f\"Category: {metadata['category']} | Topic: {metadata['topic']}\")\n",
    "        print(f\"Text: {doc[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.1: Try Your Own Queries\n",
    "\n",
    "Experiment with different queries to see how semantic search works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try your own queries here\n",
    "my_query = \"YOUR QUERY HERE\"\n",
    "\n",
    "# Uncomment to run:\n",
    "# results = search_knowledge_base(my_query, n_results=3)\n",
    "# for i, doc in enumerate(results['documents'][0]):\n",
    "#     print(f\"\\nResult {i+1}: {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Building a Simple RAG System\n",
    "\n",
    "Now let's combine retrieval with generation using an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAG system ready with Groq!\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Groq client\n",
    "client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "\n",
    "def rag_query(question: str, n_results: int = 3, model: str = \"llama-3.3-70b-versatile\"):\n",
    "    \"\"\"\n",
    "    Perform RAG: Retrieve relevant documents and generate an answer using Groq.\n",
    "    \n",
    "    Args:\n",
    "        question: User's question\n",
    "        n_results: Number of documents to retrieve\n",
    "        model: Groq model to use (options: \"llama-3.3-70b-versatile\", \"llama-3.3-8b-instant\", \n",
    "               \"mixtral-8x7b-32768\", \"gemma2-9b-it\")\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with answer and sources\n",
    "    \"\"\"\n",
    "    # Step 1: Retrieve relevant documents\n",
    "    print(f\"üîç Retrieving relevant documents...\")\n",
    "    results = search_knowledge_base(question, n_results=n_results)\n",
    "    \n",
    "    # Extract documents and metadata\n",
    "    retrieved_docs = results['documents'][0]\n",
    "    retrieved_metadata = results['metadatas'][0]\n",
    "    \n",
    "    # Step 2: Build context from retrieved documents\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"Document {i+1} ({meta['category']} - {meta['topic']}):\\n{doc}\"\n",
    "        for i, (doc, meta) in enumerate(zip(retrieved_docs, retrieved_metadata))\n",
    "    ])\n",
    "    \n",
    "    print(f\"üìö Retrieved {len(retrieved_docs)} documents\")\n",
    "    \n",
    "    # Step 3: Create prompt with context\n",
    "    prompt = f\"\"\"You are a helpful AI assistant. Answer the question based on the provided context.\n",
    "If the context doesn't contain enough information, say so.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # Step 4: Generate answer using Groq LLM\n",
    "    print(f\"ü§ñ Generating answer with Groq ({model})...\")\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful AI assistant that answers questions based on provided context.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.3,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    \n",
    "    answer = response.choices[0].message.content\n",
    "    \n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"sources\": [\n",
    "            {\"text\": doc, \"metadata\": meta}\n",
    "            for doc, meta in zip(retrieved_docs, retrieved_metadata)\n",
    "        ]\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ RAG system ready with Groq!\")\n",
    "\n",
    "# Available Groq models:\n",
    "# - llama-3.1-70b-versatile (recommended for best quality)\n",
    "# - llama-3.1-8b-instant (fastest)\n",
    "# - mixtral-8x7b-32768 (good balance)\n",
    "# - gemma2-9b-it (Google's model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "‚ùì Question: What is the difference between machine learning and deep learning?\n",
      "======================================================================\n",
      "\n",
      "üîç Retrieving relevant documents...\n",
      "üìö Retrieved 3 documents\n",
      "ü§ñ Generating answer with Groq (llama-3.3-70b-versatile)...\n",
      "\n",
      "üí° Answer:\n",
      "Based on the provided context, the difference between machine learning and deep learning is that machine learning is a broader subset of artificial intelligence that enables systems to learn and improve from experience, whereas deep learning is a subset of machine learning that specifically uses neural networks with multiple layers to learn hierarchical representations of data.\n",
      "\n",
      "In other words, all deep learning is machine learning, but not all machine learning is deep learning. Deep learning is a more specialized approach within the machine learning field, particularly effective for tasks like image recognition and natural language processing.\n",
      "\n",
      "üìñ Sources Used:\n",
      "  1. Deep Learning - Neural Networks\n",
      "  2. ML Basics - Introduction\n",
      "  3. NLP - Language Understanding\n",
      "\n",
      "======================================================================\n",
      "‚ùì Question: How can I reduce hallucinations in language models?\n",
      "======================================================================\n",
      "\n",
      "üîç Retrieving relevant documents...\n",
      "üìö Retrieved 3 documents\n",
      "ü§ñ Generating answer with Groq (llama-3.3-70b-versatile)...\n",
      "\n",
      "üí° Answer:\n",
      "To reduce hallucinations in language models, you can use the RAG (Retrieval-Augmented Generation) approach, which combines information retrieval with text generation. This approach retrieves relevant documents from a knowledge base and uses them as context for generating accurate, grounded responses, thereby reducing hallucinations in Large Language Models (LLMs).\n",
      "\n",
      "üìñ Sources Used:\n",
      "  1. Advanced AI - RAG\n",
      "  2. NLP - Language Understanding\n",
      "  3. Deep Learning - Transformers\n",
      "\n",
      "======================================================================\n",
      "‚ùì Question: What are transformers and why are they important?\n",
      "======================================================================\n",
      "\n",
      "üîç Retrieving relevant documents...\n",
      "üìö Retrieved 3 documents\n",
      "ü§ñ Generating answer with Groq (llama-3.3-70b-versatile)...\n",
      "\n",
      "üí° Answer:\n",
      "Transformers are a type of neural network architecture that has revolutionized Natural Language Processing (NLP). They are important because they use self-attention mechanisms to process sequential data in parallel, making them more efficient than previous architectures like Recurrent Neural Networks (RNNs). This allows for significant advancements in tasks such as language understanding and generation, as seen in models like GPT and BERT.\n",
      "\n",
      "üìñ Sources Used:\n",
      "  1. Deep Learning - Transformers\n",
      "  2. Deep Learning - Neural Networks\n",
      "  3. Advanced AI - RAG\n"
     ]
    }
   ],
   "source": [
    "# Test RAG with sample questions\n",
    "questions = [\n",
    "    \"What is the difference between machine learning and deep learning?\",\n",
    "    \"How can I reduce hallucinations in language models?\",\n",
    "    \"What are transformers and why are they important?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"‚ùì Question: {question}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    result = rag_query(question)\n",
    "    \n",
    "    print(f\"\\nüí° Answer:\\n{result['answer']}\")\n",
    "    \n",
    "    print(f\"\\nüìñ Sources Used:\")\n",
    "    for i, source in enumerate(result['sources']):\n",
    "        print(f\"  {i+1}. {source['metadata']['category']} - {source['metadata']['topic']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Key Takeaways\n",
    "\n",
    "### What We Learned:\n",
    "1. **Embeddings** convert text to vectors that capture semantic meaning\n",
    "2. **Vector databases** enable fast similarity search\n",
    "3. **RAG** combines retrieval + generation for accurate, grounded responses\n",
    "4. **Metadata filtering** allows precise document retrieval\n",
    "\n",
    "### Next Steps:\n",
    "- Experiment with different embedding models\n",
    "- Try chunking strategies for long documents\n",
    "- Explore advanced RAG patterns (GraphRAG, Agentic RAG)\n",
    "- Build domain-specific knowledge bases"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml-session",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
