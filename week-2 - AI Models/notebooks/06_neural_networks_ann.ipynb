{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 — Neural Networks (ANN, PyTorch) — CPU Friendly\n",
    "\n",
    "Objectives:\n",
    "- Build a small MLP in PyTorch for tabular classification (Breast Cancer dataset)\n",
    "- Practice scaling/encoding, train/val/test splits, early stopping, regularization (dropout/weight decay)\n",
    "- Evaluate with accuracy and ROC-AUC; compare to tree-based baselines conceptually\n",
    "\n",
    "Assumptions:\n",
    "- Nonlinear relationships may exist that linear or simple rules miss\n",
    "- Enough data and proper regularization to avoid overfitting\n",
    "\n",
    "Cautions / Data Prep:\n",
    "- Always scale/normalize numeric features for neural networks\n",
    "- One-hot encode categoricals (not needed here)\n",
    "- Watch class imbalance; consider `pos_weight` or rebalancing\n",
    "- Split train/val/test to prevent information leakage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid', context='notebook')\n",
    "np.random.seed(42)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cpu')  # CPU-only per course constraints\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Load dataset and split: train/val/test\n",
    "Use a validation set for early stopping. Keep test set untouched for final evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target, name='target')  # 0/1\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.2, random_state=42, stratify=y_train_full\n",
    ")\n",
    "X.shape, y.value_counts(normalize=True).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale features on train only, then apply to val/test to avoid leakage. Convert to tensors and build loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_val_s   = scaler.transform(X_val)\n",
    "X_test_s  = scaler.transform(X_test)\n",
    "\n",
    "X_train_t = torch.tensor(X_train_s, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train.values.reshape(-1, 1), dtype=torch.float32)\n",
    "X_val_t   = torch.tensor(X_val_s, dtype=torch.float32)\n",
    "y_val_t   = torch.tensor(y_val.values.reshape(-1, 1), dtype=torch.float32)\n",
    "X_test_t  = torch.tensor(X_test_s, dtype=torch.float32)\n",
    "y_test_t  = torch.tensor(y_test.values.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "val_ds   = TensorDataset(X_val_t, y_val_t)\n",
    "test_ds  = TensorDataset(X_test_t, y_test_t)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=256, shuffle=False)\n",
    "test_loader  = DataLoader(test_ds, batch_size=256, shuffle=False)\n",
    "X_train_t.shape, y_train_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Define a small MLP with dropout\n",
    "Binary classification via `BCEWithLogitsLoss` (logits out; apply sigmoid at evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden=64, p_drop=0.2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p_drop),\n",
    "            nn.Linear(hidden, hidden//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p_drop),\n",
    "            nn.Linear(hidden//2, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "in_dim = X_train_t.shape[1]\n",
    "model = MLP(in_dim=in_dim, hidden=64, p_drop=0.2).to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)  # weight decay as L2 reg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions: train one epoch, evaluate on a loader (loss, accuracy, ROC-AUC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_model(model, loader):\n",
    "    model.eval()\n",
    "    losses, probs, targets = [], [], []\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        losses.append(loss.item())\n",
    "        p = torch.sigmoid(logits).cpu().numpy().ravel()\n",
    "        t = yb.cpu().numpy().ravel()\n",
    "        probs.append(p); targets.append(t)\n",
    "    probs = np.concatenate(probs)\n",
    "    targets = np.concatenate(targets)\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "    acc = accuracy_score(targets, preds)\n",
    "    try:\n",
    "        auc = roc_auc_score(targets, probs)\n",
    "    except ValueError:\n",
    "        auc = np.nan\n",
    "    return np.mean(losses), acc, auc\n",
    "\n",
    "def train_one_epoch(model, loader):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    return np.mean(losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Train with early stopping (by validation AUC)\n",
    "Keep epochs small for CPU and stop when val AUC stops improving for a few rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "patience = 5\n",
    "best_auc = -np.inf\n",
    "pat = 0\n",
    "hist = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'val_auc': []}\n",
    "\n",
    "for ep in range(1, epochs+1):\n",
    "    tr_loss = train_one_epoch(model, train_loader)\n",
    "    vl_loss, vl_acc, vl_auc = eval_model(model, val_loader)\n",
    "    hist['train_loss'].append(tr_loss)\n",
    "    hist['val_loss'].append(vl_loss)\n",
    "    hist['val_acc'].append(vl_acc)\n",
    "    hist['val_auc'].append(vl_auc)\n",
    "    print(f\"Epoch {ep:02d} | train_loss={tr_loss:.4f} val_loss={vl_loss:.4f} val_acc={vl_acc:.3f} val_auc={vl_auc:.3f}\")\n",
    "    if vl_auc > best_auc + 1e-4:\n",
    "        best_auc = vl_auc\n",
    "        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "        pat = 0\n",
    "    else:\n",
    "        pat += 1\n",
    "        if pat >= patience:\n",
    "            print(f\"Early stopping at epoch {ep} (best val AUC={best_auc:.3f})\")\n",
    "            break\n",
    "\n",
    "# Restore best weights\n",
    "model.load_state_dict(best_state)\n",
    "best_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(hist['train_loss'], label='train')\n",
    "plt.plot(hist['val_loss'], label='val')\n",
    "plt.title('Loss'); plt.legend()\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(hist['val_auc'], label='val AUC')\n",
    "plt.title('Validation AUC'); plt.legend()\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Test evaluation\n",
    "Report accuracy, ROC-AUC, and classification report on the test set (held out entirely)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl, ta, tu = eval_model(model, test_loader)\n",
    "print({'test_loss': round(tl, 4), 'test_acc': round(ta, 3), 'test_auc': round(tu, 3)})\n",
    "\n",
    "# Classification report\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(X_test_t.to(device))\n",
    "    probs = torch.sigmoid(logits).cpu().numpy().ravel()\n",
    "preds = (probs >= 0.5).astype(int)\n",
    "print(classification_report(y_test.values, preds, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: On tabular data, tree ensembles (Random Forest / XGBoost) often outperform simple MLPs. ANNs shine with extensive feature engineering, large data, or when strong nonlinearity exists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "Instructor solution cells are hidden/collapsed.\n",
    "1. Scaling ablation: Retrain the model without scaling (use raw `X_train.values`)—observe convergence and test AUC differences.\n",
    "2. Regularization: Increase dropout to 0.5 and/or weight_decay to 1e-3. Does validation AUC improve or degrade?\n",
    "3. Class imbalance: Compute `pos_weight = (N_neg/N_pos)` from training labels and pass it to `BCEWithLogitsLoss(pos_weight=...)`. Compare metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "tags": ["exercise"] },
   "outputs": [],
   "source": [
    "# Exercise 1: Remove scaling\n",
    "# TODO: Build new tensors/loaders from raw X_train/X_val/X_test without StandardScaler and retrain a small model (10-20 epochs).\n",
    "# Compare test AUC.\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "tags": ["solution"], "jupyter": { "source_hidden": true } },
   "outputs": [],
   "source": [
    "# Solution 1 (hidden)\n",
    "Xtr_raw = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "Xva_raw = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "Xte_raw = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "ytr = torch.tensor(y_train.values.reshape(-1,1), dtype=torch.float32)\n",
    "yva = torch.tensor(y_val.values.reshape(-1,1), dtype=torch.float32)\n",
    "yte = torch.tensor(y_test.values.reshape(-1,1), dtype=torch.float32)\n",
    "\n",
    "trL = DataLoader(TensorDataset(Xtr_raw, ytr), batch_size=64, shuffle=True)\n",
    "vaL = DataLoader(TensorDataset(Xva_raw, yva), batch_size=256)\n",
    "teL = DataLoader(TensorDataset(Xte_raw, yte), batch_size=256)\n",
    "\n",
    "m2 = MLP(in_dim=Xtr_raw.shape[1], hidden=64, p_drop=0.2)\n",
    "opt2 = torch.optim.Adam(m2.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "crit2 = nn.BCEWithLogitsLoss()\n",
    "best = -np.inf; state=None; pat=0\n",
    "for ep in range(15):\n",
    "    m2.train(); ls=[]\n",
    "    for xb,yb in trL:\n",
    "        opt2.zero_grad(); out = m2(xb); loss = crit2(out, yb); loss.backward(); opt2.step(); ls.append(loss.item())\n",
    "    vl = eval_model(m2, vaL)[-1]\n",
    "    if vl>best: best=vl; state={k:v.clone() for k,v in m2.state_dict().items()}; pat=0\n",
    "    else: pat+=1\n",
    "    if pat>=3: break\n",
    "m2.load_state_dict(state)\n",
    "test_auc_raw = eval_model(m2, teL)[-1]\n",
    "round(test_auc_raw,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "tags": ["exercise"] },
   "outputs": [],
   "source": [
    "# Exercise 2: Regularization sweep\n",
    "# TODO: Try dropout=0.5 and/or weight_decay=1e-3 in the optimizer; record val/test AUC.\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "tags": ["solution"], "jupyter": { "source_hidden": true } },
   "outputs": [],
   "source": [
    "# Solution 2 (hidden)\n",
    "def run_conf(pdrop=0.5, wd=1e-3):\n",
    "    m = MLP(in_dim=in_dim, hidden=64, p_drop=pdrop)\n",
    "    opt = torch.optim.Adam(m.parameters(), lr=1e-3, weight_decay=wd)\n",
    "    best=-np.inf; st=None; pat=0\n",
    "    for ep in range(20):\n",
    "        m.train();\n",
    "        for xb,yb in train_loader:\n",
    "            opt.zero_grad(); out=m(xb); loss=criterion(out, yb); loss.backward(); opt.step()\n",
    "        va_auc = eval_model(m, val_loader)[-1]\n",
    "        if va_auc>best: best=va_auc; st={k:v.clone() for k,v in m.state_dict().items()}; pat=0\n",
    "        else:\n",
    "            pat+=1\n",
    "            if pat>=4: break\n",
    "    m.load_state_dict(st)\n",
    "    return eval_model(m, test_loader)[-1]\n",
    "\n",
    "out_a = run_conf(0.5, 1e-3)\n",
    "out_b = run_conf(0.2, 1e-3)\n",
    "out_c = run_conf(0.5, 1e-4)\n",
    "{'drop0.5_wd1e-3': round(out_a,3), 'drop0.2_wd1e-3': round(out_b,3), 'drop0.5_wd1e-4': round(out_c,3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "tags": ["exercise"] },
   "outputs": [],
   "source": [
    "# Exercise 3: Class imbalance with pos_weight\n    ",
    "# TODO: Compute pos_weight = N_neg/N_pos on the training labels and pass it to BCEWithLogitsLoss.\n",
    "# Retrain briefly (10-15 epochs) and compare test AUC.\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "tags": ["solution"], "jupyter": { "source_hidden": true } },
   "outputs": [],
   "source": [
    "# Solution 3 (hidden)\n",
    "pos = (y_train.values==1).sum(); neg = (y_train.values==0).sum()\n",
    "pw = torch.tensor([neg/pos], dtype=torch.float32)\n",
    "m3 = MLP(in_dim=in_dim, hidden=64, p_drop=0.2)\n",
    "crit3 = nn.BCEWithLogitsLoss(pos_weight=pw)\n",
    "opt3 = torch.optim.Adam(m3.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "best=-np.inf; st=None; pat=0\n",
    "for ep in range(15):\n",
    "    m3.train()\n",
    "    for xb,yb in train_loader:\n",
    "        opt3.zero_grad(); out=m3(xb); loss=crit3(out, yb); loss.backward(); opt3.step()\n",
    "    val_auc = eval_model(m3, val_loader)[-1]\n",
    "    if val_auc>best: best=val_auc; st={k:v.clone() for k,v in m3.state_dict().items()}; pat=0\n",
    "    else:\n",
    "        pat+=1\n",
    "        if pat>=3: break\n",
    "m3.load_state_dict(st)\n",
    "auc3 = eval_model(m3, test_loader)[-1]\n",
    "round(auc3,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-up checklist\n",
    "- [ ] Scale numeric features and split train/val/test\n",
    "- [ ] Use early stopping to limit overfitting\n",
    "- [ ] Try dropout and weight decay; tune hidden sizes\n",
    "- [ ] Consider class imbalance via pos_weight or sampling\n",
    "- [ ] Track both accuracy and ROC-AUC for classification\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": { "display_name": "Python 3", "language": "python", "name": "python3" },
  "language_info": { "name": "python" }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
