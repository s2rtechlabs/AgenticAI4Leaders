{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07 â€” CNNs (PyTorch) + Optional Transfer Learning (CPU-friendly)\n",
    "\n",
    "Objectives:\n",
    "- Build and train a simple CNN on Fashion-MNIST (CPU-friendly)\n",
    "- Use normalization and light augmentation; track accuracy and loss\n",
    "- Discuss overfitting and mitigations (augmentation, dropout)\n",
    "- Optional: Demonstrate transfer learning with a pretrained ResNet-18 as a frozen feature extractor (tiny subset, CPU-safe)\n",
    "\n",
    "Assumptions:\n",
    "- Local connectivity and translational invariance are useful for images\n",
    "- Inputs are tensors with standardized shape and normalized pixel values\n",
    "\n",
    "Cautions/Data Prep:\n",
    "- Normalize pixel values (e.g., mean/std) and keep consistent pre-processing\n",
    "- Data augmentation can help generalization but must be reasonable\n",
    "- Training deep models from scratch on CPU is slow; keep models small and epochs few\n",
    "- Transfer learning on CPU is feasible for small subsets; keep it optional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid', context='notebook')\n",
    "np.random.seed(42)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms, models\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "device = torch.device('cpu')  # CPU-only per course constraints\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Dataset and transforms (Fashion-MNIST)\n",
    "Normalize to mean=0.5, std=0.5 (or dataset-specific stats). Add light augmentation for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = (0.5,), (0.5,)\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomCrop(28, padding=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "test_tfms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "root = './_data'\n",
    "train_ds = datasets.FashionMNIST(root, train=True, download=True, transform=train_tfms)\n",
    "test_ds  = datasets.FashionMNIST(root, train=False, download=True, transform=test_tfms)\n",
    "\n",
    "# Create a small validation split from the training set (e.g., 5k val)\n",
    "val_size = 5000\n",
    "indices = torch.randperm(len(train_ds)).tolist()\n",
    "val_idx, tr_idx = indices[:val_size], indices[val_size:]\n",
    "val_ds = Subset(datasets.FashionMNIST(root, train=True, download=False, transform=test_tfms), val_idx)\n",
    "tr_ds  = Subset(train_ds, tr_idx)\n",
    "\n",
    "batch_size = 128\n",
    "tr_loader = DataLoader(tr_ds, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "te_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "len(tr_ds), len(val_ds), len(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize a few samples to confirm transforms and labels look reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']\n",
    "xb, yb = next(iter(tr_loader))\n",
    "grid = xb[:16].clone()\n",
    "# de-normalize for plotting\n",
    "grid = torch.clamp(grid * std[0] + mean[0], 0, 1)\n",
    "fig, axes = plt.subplots(4,4, figsize=(6,6))\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    ax.imshow(grid[i,0].numpy(), cmap='gray')\n",
    "    ax.set_title(classes[yb[i].item()], fontsize=8)\n",
    "    ax.axis('off')\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Simple CNN model\n",
    "Small architecture suitable for CPU training within a few epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10, p_drop=0.25):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # 14x14\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2)   # 7x7\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p_drop),\n",
    "            nn.Linear(64*7*7, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p_drop),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "model = SmallCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, loader, optimizer=None):\n",
    "    is_train = optimizer is not None\n",
    "    model.train() if is_train else model.eval()\n",
    "    losses, correct, total = [], 0, 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        if is_train:\n",
    "            optimizer.zero_grad()\n",
    "        out = model(xb)\n",
    "        loss = criterion(out, yb)\n",
    "        if is_train:\n",
    "            loss.backward(); optimizer.step()\n", 
    "        losses.append(loss.item())\n",
    "        preds = out.argmax(dim=1)\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total += yb.size(0)\n",
    "    return np.mean(losses), correct/total\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_preds, all_true = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            out = model(xb.to(device))\n",
    "            preds = out.argmax(dim=1).cpu()\n",
    "            all_preds.append(preds)\n",
    "            all_true.append(yb)\n",
    "    y_pred = torch.cat(all_preds).numpy()\n",
    "    y_true = torch.cat(all_true).numpy()\n",
    "    return y_true, y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Train a few epochs (CPU-friendly)\n",
    "Keep epochs small; use validation to track generalization and stop early if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "hist = {'tr_loss':[], 'tr_acc':[], 'val_loss':[], 'val_acc':[]}\n",
    "best_val = -np.inf\n",
    "best_state = None\n",
    "patience, pat = 2, 0\n",
    "\n",
    "for ep in range(1, epochs+1):\n",
    "    tl, ta = run_epoch(model, tr_loader, optimizer)\n",
    "    vl, va = run_epoch(model, val_loader)\n",
    "    hist['tr_loss'].append(tl); hist['tr_acc'].append(ta)\n",
    "    hist['val_loss'].append(vl); hist['val_acc'].append(va)\n",
    "    print(f\"Epoch {ep:02d} | train_loss={tl:.4f} acc={ta:.3f} | val_loss={vl:.4f} acc={va:.3f}\")\n",
    "    if va > best_val + 1e-4:\n",
    "        best_val = va\n",
    "        best_state = {k: v.cpu().clone() for k,v in model.state_dict().items()}\n",
    "        pat = 0\n",
    "    else:\n",
    "        pat += 1\n",
    "        if pat >= patience:\n",
    "            print(f\"Early stopping at epoch {ep} (best val acc={best_val:.3f})\")\n",
    "            break\n",
    "\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "plt.figure(figsize=(9,3))\n",
    "plt.subplot(1,2,1); plt.plot(hist['tr_loss'], label='train'); plt.plot(hist['val_loss'], label='val'); plt.title('Loss'); plt.legend()\n",
    "plt.subplot(1,2,2); plt.plot(hist['tr_acc'], label='train'); plt.plot(hist['val_acc'], label='val'); plt.title('Accuracy'); plt.legend()\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on the test set and show a confusion matrix + classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred = evaluate(model, te_loader)\n",
    "print(classification_report(y_true, y_pred, target_names=classes, digits=3))\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=False, cmap='Blues')\n",
    "plt.title('Confusion Matrix (Test)'); plt.xlabel('Predicted'); plt.ylabel('True')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Augmentation ablation (brief)\n",
    "Compare with no augmentation for a couple of epochs to see effect on validation accuracy (may vary by seed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": ["exercise"]
   },
   "outputs": [],
   "source": [
    "# Exercise: Remove augmentation and compare\n",
    "# TODO: Build a new training set with transforms.ToTensor()+Normalize only, train 2 epochs, and compare val acc.\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": ["solution"],
    "jupyter": { "source_hidden": true }
   },
   "outputs": [],
   "source": [
    "# Solution (hidden)\n",
    "train_noaug = datasets.FashionMNIST(root, train=True, download=False, transform=test_tfms)\n",
    "train_noaug = Subset(train_noaug, tr_idx)\n",
    "loader_noaug = DataLoader(train_noaug, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "m2 = SmallCNN().to(device)\n",
    "opt2 = torch.optim.Adam(m2.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "for ep in range(2):\n",
    "    tl, ta = run_epoch(m2, loader_noaug, opt2)\n",
    "    vl, va = run_epoch(m2, val_loader)\n",
    "va"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Optional: Transfer learning with ResNet-18 (feature extractor)\n",
    "Use an ImageNet-pretrained ResNet-18 as a frozen feature extractor. We adapt Fashion-MNIST (1 channel) to 3 channels and resize to 224. For CPU, we only use a small subset for training to keep runtime short. Accuracy is illustrative, not state-of-the-art.\n",
    "\n",
    "Warning: Downloading pretrained weights requires internet and some minutes on CPU for feature extraction even on small subsets. Keep sample sizes small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": ["optional"]
   },
   "outputs": [],
   "source": [
    "opt_tfms = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.Grayscale(num_output_channels=3),  # expand to 3 channels\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "train_opt = datasets.FashionMNIST(root, train=True, download=False, transform=opt_tfms)\n",
    "test_opt  = datasets.FashionMNIST(root, train=False, download=False, transform=opt_tfms)\n",
    "\n",
    "# tiny subset to keep CPU runtime small\n",
    "train_small_idx = torch.randperm(len(train_opt))[:1500]\n",
    "val_small_idx   = torch.randperm(len(train_opt))[1500:2000]\n",
    "test_small_idx  = torch.randperm(len(test_opt))[:2000]\n",
    "\n",
    "train_small = Subset(train_opt, train_small_idx)\n",
    "val_small   = Subset(train_opt, val_small_idx)\n",
    "test_small  = Subset(test_opt,  test_small_idx)\n",
    "\n",
    "trL = DataLoader(train_small, batch_size=64, shuffle=True, num_workers=2)\n",
    "vaL = DataLoader(val_small, batch_size=128, shuffle=False, num_workers=2)\n",
    "teL = DataLoader(test_small, batch_size=128, shuffle=False, num_workers=2)\n",
    "\n",
    "resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "for p in resnet.parameters():\n",
    "    p.requires_grad = False\n",
    "resnet.fc = nn.Linear(resnet.fc.in_features, 10)  # classifier head\n",
    "resnet = resnet.to(device)\n",
    "\n",
    "opt = torch.optim.Adam(resnet.fc.parameters(), lr=1e-3)\n",
    "\n",
    "def train_head(model, trL, vaL, epochs=2):  # keep very small\n",
    "    best, st = -np.inf, None\n",
    "    for ep in range(1, epochs+1):\n",
    "        tl, ta = run_epoch(model, trL, opt)\n",
    "        vl, va = run_epoch(model, vaL)\n",
    "        print(f\"[ResNet head] epoch {ep} | train_acc={ta:.3f} | val_acc={va:.3f}\")\n",
    "        if va>best: best, st = va, {k:v.cpu().clone() for k,v in model.state_dict().items()}\n",
    "    if st: model.load_state_dict(st)\n",
    "    return best\n",
    "\n",
    "best_va = train_head(resnet, trL, vaL, epochs=2)\n",
    "y_true_r, y_pred_r = evaluate(resnet, teL)\n",
    "print(classification_report(y_true_r, y_pred_r, target_names=classes, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "Instructor solution cells are hidden/collapsed.\n",
    "1. Dropout sweep: Try `p_drop` in [0.0, 0.25, 0.5]; train 3 epochs; compare validation accuracy and test report.\n",
    "2. Augmentation variants: Replace RandomCrop with RandomAffine (small rotations) and compare.\n",
    "3. Optional ResNet: Unfreeze the last block (layer4) and train for 1 extra epoch with very small LR (e.g., 1e-4). Does val acc improve?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "tags": ["exercise"] },
   "outputs": [],
   "source": [
    "# Exercise 1: Dropout sweep\n",
    "# TODO: Rebuild SmallCNN with p_drop in [0.0, 0.25, 0.5] and record val acc after 3 epochs.\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "tags": ["solution"], "jupyter": { "source_hidden": true } },
   "outputs": [],
   "source": [
    "# Solution 1 (hidden)\n",
    "vals = {}\n",
    "for p in [0.0, 0.25, 0.5]:\n",
    "    m = SmallCNN(p_drop=p).to(device)\n",
    "    optm = torch.optim.Adam(m.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    best=-np.inf; st=None\n",
    "    for _ in range(3):\n",
    "        run_epoch(m, tr_loader, optm)\n",
    "        v = run_epoch(m, val_loader)[1]\n",
    "        if v>best: best=v; st={k:v_.cpu().clone() for k,v_ in m.state_dict().items()}\n",
    "    if st: m.load_state_dict(st)\n",
    "    vals[p] = best\n",
    "vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "tags": ["exercise"] },
   "outputs": [],
   "source": [
    "# Exercise 2: Augmentation variant\n",
    "# TODO: Replace RandomCrop with transforms.RandomAffine(degrees=10, translate=(0.05,0.05)) and compare val acc after 3 epochs.\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "tags": ["solution"], "jupyter": { "source_hidden": true } },
   "outputs": [],
   "source": [
    "# Solution 2 (hidden)\n",
    "aug2 = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.05,0.05)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "train_aug2 = datasets.FashionMNIST(root, train=True, download=False, transform=aug2)\n",
    "train_aug2 = Subset(train_aug2, tr_idx)\n",
    "loader_aug2 = DataLoader(train_aug2, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "m = SmallCNN().to(device)\n",
    "optm = torch.optim.Adam(m.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "best=-np.inf\n",
    "for _ in range(3):\n",
    "    run_epoch(m, loader_aug2, optm)\n",
    "    best = max(best, run_epoch(m, val_loader)[1])\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "tags": ["exercise", "optional"] },
   "outputs": [],
   "source": [
    "# Exercise 3 (Optional): Unfreeze part of ResNet\n",
    "# TODO: Set requires_grad=True for layer4 parameters, use LR=1e-4, and train 1 epoch on the small subset; check val acc.\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "tags": ["solution", "optional"], "jupyter": { "source_hidden": true } },
   "outputs": [],
   "source": [
    "# Solution 3 (hidden, optional)\n",
    "try:\n",
    "    for p in resnet.layer4.parameters():\n",
    "        p.requires_grad = True\n",
    "    opt_small = torch.optim.Adam(filter(lambda p: p.requires_grad, resnet.parameters()), lr=1e-4)\n",
    "    # quick 1 epoch fine-tune\n",
    "    run_epoch(resnet, trL, opt_small)\n",
    "    va_acc = run_epoch(resnet, vaL)[1]\n",
    "    va_acc\n",
    "except NameError:\n",
    "    print('ResNet section was not run. Execute the optional cell above first.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-up checklist\n",
    "- [ ] Normalize inputs consistently across train/val/test\n",
    "- [ ] Keep CNN architectures small for CPU; limit epochs\n",
    "- [ ] Use augmentation and dropout to reduce overfitting\n",
    "- [ ] Consider frozen backbones for transfer learning on small datasets\n",
    "- [ ] Inspect errors via confusion matrix per class\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": { "display_name": "Python 3", "language": "python", "name": "python3" },
  "language_info": { "name": "python" }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
