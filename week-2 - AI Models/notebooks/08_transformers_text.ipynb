{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08 — Transformers for Text (PyTorch + Hugging Face) — Inference-first (CPU)\n",
    "\n",
    "Objectives:\n",
    "- Use Hugging Face pipelines for quick text classification (sentiment) on CPU\n",
    "- Understand tokenization, truncation, and context window limits\n",
    "- Batch inference efficiently and evaluate accuracy on a small validation split\n",
    "- Discuss privacy, bias, and compliance considerations\n",
    "\n",
    "Assumptions:\n",
    "- Input is tokenized text sequences; model considers relationships across positions (self-attention)\n",
    "- Context window is finite (e.g., 512 tokens) — longer text must be truncated or chunked\n",
    "\n",
    "Cautions/Data Prep:\n",
    "- Clean/tokenize appropriately; watch special tokens and truncation behavior\n",
    "- Ensure input length ≤ model max length; otherwise truncate/slide window\n",
    "- Training/fine-tuning requires significant compute and data; avoid in CPU-only short labs\n",
    "- Handle privacy: avoid sensitive data; consider anonymization and auditability\n",
    "- Monitor for bias and harmful outputs in enterprise contexts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import perf_counter\n",
    "from pprint import pprint\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "\n",
    "MODEL = 'distilbert-base-uncased-finetuned-sst-2-english'  # small, CPU-friendly sentiment model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Quickstart: Sentiment analysis pipeline\n",
    "Pipelines wrap tokenization + model forward + postprocessing. First call downloads weights (cached afterwards)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = pipeline('sentiment-analysis', model=MODEL)\n",
    "texts = [\n",
    "    \"I loved the movie! The performances were outstanding.\",\n",
    "    \"This was a waste of time. Completely boring and predictable.\",\n",
    "]\n",
    "pprint(clf(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood: tokenization, truncation, and max sequence length. Inspect tokenizer and model config (e.g., max length ~512 tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = AutoTokenizer.from_pretrained(MODEL)\n",
    "mdl = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "tok.model_max_length, mdl.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual tokenization example: encode/decode, attention masks, and truncation vs no truncation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"This product is surprisingly good and well worth the price!\" * 30  # long\n",
    "enc_trunc = tok(sample, truncation=True, max_length=128, padding='max_length', return_tensors='pt')\n",
    "enc_no_trunc = tok(sample, truncation=False, return_tensors='pt')\n",
    "len(enc_trunc['input_ids'][0]), enc_no_trunc['input_ids'].shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pass tokenizer kwargs through the pipeline call to control truncation/max_length and batch size for speed on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [\n",
    "    \"Terrific pacing and solid acting.\",\n",
    "    \"Mediocre plot, but some scenes were fun.\",\n",
    "    \"I wouldn't recommend this to anyone.\",\n",
    "    \"Absolutely fantastic!\", \n",
    "] * 64  # simulate a larger batch\n",
    "\n",
    "t0 = perf_counter()\n",
    "out = clf(batch, batch_size=32, truncation=True, max_length=128)\n",
    "dt = perf_counter() - t0\n",
    "print(f\"Processed {len(batch)} texts in {dt:.2f}s (~{len(batch)/dt:.1f} texts/s on CPU)\")\n",
    "out[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Evaluate on a standard dataset (SST-2 validation subset)\n",
    "We use GLUE SST-2 dev split (binary sentiment) and evaluate pipeline accuracy on the first N examples for a quick CPU exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('glue', 'sst2', split='validation')  # requires internet to fetch\n",
    "N = 200  # keep small for CPU\n",
    "sampled = ds.select(range(N))\n",
    "label_map = {0: 'NEGATIVE', 1: 'POSITIVE'}\n",
    "\n",
    "preds = clf(sampled['sentence'], batch_size=32, truncation=True, max_length=128)\n",
    "pred_labels = [p['label'] for p in preds]\n",
    "true_labels = [label_map[y] for y in sampled['label']]\n",
    "acc = np.mean(np.array(pred_labels) == np.array(true_labels))\n",
    "print({'N': N, 'accuracy': round(float(acc), 3)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truncation effect: vary max_length and compare accuracy stability. Longer sequences may capture more context at the cost of runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = {}\n",
    "for L in [64, 128, 256]:\n",
    "    preds_L = clf(sampled['sentence'], batch_size=32, truncation=True, max_length=L)\n",
    "    pred_L = [p['label'] for p in preds_L]\n",
    "    accs[L] = float(np.mean(np.array(pred_L) == np.array(true_labels)))\n",
    "accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Handling long texts (chunking)\n",
    "For texts longer than the model max length, chunk and aggregate predictions (e.g., average logits or majority vote of labels). Here, perform a simple majority vote over chunks for sentiment polarity as a demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, tokenizer, max_length=256, stride=32):\n",
    "    tokens = tokenizer(text, return_offsets_mapping=True, add_special_tokens=False)\n",
    "    ids = tokens['input_ids']\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        chunk_ids = ids[i:i+max_length-2]  # reserve for [CLS]/[SEP]\n",
    "        chunk_text = tokenizer.decode(chunk_ids)\n",
    "        chunks.append(chunk_text)\n",
    "        i += max_length - 2 - stride\n",
    "        if i <= 0: i = len(chunk_ids)  # safety\n",
    "    return chunks\n",
    "\n",
    "long_text = (\"The film starts strong with engaging characters and witty dialogue, \"\n",
    "             \"but gradually loses momentum, suffering from pacing issues. \"\n",
    "             \"Nonetheless, certain scenes are genuinely moving, and the score is memorable. \"\n",
    "             \"Overall, it's a mixed bag with both charming moments and frustrating flaws.\") * 20\n",
    "\n",
    "parts = chunk_text(long_text, tok, max_length=128, stride=32)\n",
    "pred_parts = clf(parts, batch_size=16, truncation=True, max_length=128)\n",
    "labels = [p['label'] for p in pred_parts]\n",
    "final = pd.Series(labels).value_counts().idxmax()\n",
    "final, pd.Series(labels).value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Notes on privacy, bias, and compliance\n",
    "- Avoid passing sensitive data to third-party endpoints; models downloaded locally run offline after caching, but take care when logging text\n",
    "- Audit and anonymize where necessary; keep access controls and data retention policies\n",
    "- Monitor outputs for bias and harmful content; document known model limitations\n",
    "- In enterprise, add guardrails, human-in-the-loop, and domain-specific evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "Instructor solution cells are hidden/collapsed.\n",
    "1. Batch vs single: Measure throughput (texts/sec) for single-item calls in a loop vs a batched call of size 32 on 256 texts.\n",
    "2. Truncation stability: For SST-2 N=200, compute accuracy at max_length in [32, 64, 128, 256]; plot or print results. Which setting balances speed and accuracy best on CPU?\n",
    "3. Chunking policy: Implement a function that aggregates chunk predictions by averaging scores (use the `score` field), not just majority label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "tags": ["exercise"] },
   "outputs": [],
   "source": [
    "# Exercise 1: Batch vs single\n",
    "# TODO: Create 256 medium-length sentences. Time clf on: (a) loop calling one-by-one; (b) one batched call (batch_size=32).\n",
    "# Report times and throughput.\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "tags": ["solution"], "jupyter": { "source_hidden": true } },
   "outputs": [],
   "source": [
    "# Solution 1 (hidden)\n",
    "samples = [\"The movie was okay, some parts dragged but acting was fine.\" for _ in range(256)]\n",
    "t0 = perf_counter();\n",
    "single = [clf([s])[0] for s in samples];\n",
    "t_single = perf_counter() - t0\n",
    "t0 = perf_counter();\n",
    "batched = clf(samples, batch_size=32, truncation=True, max_length=128);\n",
    "t_batch = perf_counter() - t0\n",
    "{'single_s': round(t_single,2), 'batch_s': round(t_batch,2), 'throughput_single': round(256/max(t_single,1e-6),1), 'throughput_batch': round(256/max(t_batch,1e-6),1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "tags": ["exercise"] },
   "outputs": [],
   "source": [
    "# Exercise 2: Truncation stability on SST-2\n",
    "# TODO: For L in [32,64,128,256], compute accuracy on the first N=200 validation examples.\n",
    "# Hint: reuse 'sampled' and 'true_labels'.\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "tags": ["solution"], "jupyter": { "source_hidden": true } },
   "outputs": [],
   "source": [
    "# Solution 2 (hidden)\n",
    "accs2 = {}\n",
    "for L in [32,64,128,256]:\n",
    "    predsL = clf(sampled['sentence'], batch_size=32, truncation=True, max_length=L)\n",
    "    accs2[L] = float(np.mean(np.array([p['label'] for p in predsL]) == np.array(true_labels)))\n",
    "accs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "tags": ["exercise"] },
   "outputs": [],
   "source": [
    "# Exercise 3: Score-averaging chunking\n",
    "# TODO: Modify the chunk aggregation to average class scores and pick the final label by max average score.\n",
    "# Hint: create a vector [score_pos, score_neg] or similar from pipeline outputs and average across chunks.\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "tags": ["solution"], "jupyter": { "source_hidden": true } },
   "outputs": [],
   "source": [
    "# Solution 3 (hidden)\n",
    "def chunk_predict_avg(text, clf, tokenizer, max_length=128, stride=32):\n",
    "    chunks = chunk_text(text, tokenizer, max_length=max_length, stride=stride)\n",
    "    outs = clf(chunks, batch_size=16, truncation=True, max_length=max_length)\n",
    "    # map label->index\n",
    "    labels = sorted(list({o['label'] for o in outs}))\n",
    "    idx = {lab:i for i,lab in enumerate(labels)}\n",
    "    scores = np.zeros((len(outs), len(labels)))\n",
    "    for i,o in enumerate(outs):\n",
    "        scores[i, idx[o['label']]] = o['score']\n",
    "    avg = scores.mean(axis=0)\n",
    "    pred = labels[int(np.argmax(avg))]\n",
    "    return pred, {labels[i]: float(avg[i]) for i in range(len(labels))}\n",
    "\n",
    "pred_label, avg_scores = chunk_predict_avg(long_text, clf, tok)\n",
    "pred_label, avg_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-up checklist\n",
    "- [ ] Use pipelines for quick CPU inference; cache models locally\n",
    "- [ ] Control truncation and max_length; respect model context window\n",
    "- [ ] Batch inputs for speed on CPU\n",
    "- [ ] Evaluate on a small standard split to sanity-check performance\n",
    "- [ ] Consider privacy, bias, and compliance in production use\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
