{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 — Linear Regression (Hands-on)\n",
    "\n",
    "Objectives:\n",
    "- Fit a linear regression model and evaluate performance (R^2, MAE, RMSE)\n",
    "- Validate assumptions: linearity, independence, homoscedasticity, normality of residuals, and low multicollinearity\n",
    "- Use diagnostic plots (residual vs. fitted, QQ plot)\n",
    "- Apply statistical tests (Breusch–Pagan for heteroskedasticity) and VIF for multicollinearity\n",
    "- Practice feature scaling/engineering and handling outliers\n",
    "\n",
    "Assumptions to remember:\n",
    "- Linearity: relationship between features and target is linear\n",
    "- Independence: observations are independent\n",
    "- Homoscedasticity: residual variance is constant\n",
    "- Normality of Errors: residuals are approximately normal (for CI/PI validity)\n",
    "- Low Multicollinearity: predictors are not highly correlated\n",
    "\n",
    "Cautions/Data Prep:\n",
    "- Outliers can distort fit; consider removal/transformations\n",
    "- Scale features if using regularization or polynomial features\n",
    "- Handle missing data; linear regression doesn’t natively handle NaNs\n",
    "- Always check diagnostics after fitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import make_regression, fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from scipy import stats\n",
    "\n",
    "sns.set(style='whitegrid', context='notebook')\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Load a simple synthetic dataset\n",
    "We start with a clean linear signal with noise. This is fast and ideal on CPU. Optionally, try California Housing later (slower but realistic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=1000, n_features=5, n_informative=5, noise=15.0, random_state=42)\n",
    "feature_names = [f\"x{i+1}\" for i in range(X.shape[1])]\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['y'] = y\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/validation split, then fit a simple LinearRegression (no regularization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[feature_names], df['y'], test_size=0.2, random_state=42)\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = linreg.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2, mae, rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect coefficients and intercept. Scale matters when interpreting coefficients; with standardized features, magnitudes are comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df = pd.DataFrame({'feature': feature_names, 'coefficient': linreg.coef_}).sort_values('coefficient')\n",
    "display(pd.Series({'intercept': linreg.intercept_}))\n",
    "coef_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Residual diagnostics\n",
    "- Residual vs. predicted: look for randomness (no clear pattern)\n",
    "- Histogram/QQ plot: check approximate normality\n",
    "- Homoscedasticity test (Breusch–Pagan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_test - y_pred\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "axes[0].scatter(y_pred, residuals, alpha=0.6)\n",
    "axes[0].axhline(0, color='red', linestyle='--')\n",
    "axes[0].set_title('Residuals vs Predicted')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Residuals')\n",
    "\n", 
    "sns.histplot(residuals, kde=True, ax=axes[1])\n",
    "axes[1].set_title('Residuals Distribution')\n",
    "\n",
    "sm.ProbPlot(residuals, fit=True).qqplot(line='45', ax=axes[2])\n",
    "axes[2].set_title('QQ Plot of Residuals')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Normality tests (indicative)\n",
    "jb_stat, jb_p = stats.jarque_bera(residuals)\n",
    "sw_stat, sw_p = stats.shapiro(residuals)\n",
    "print(f\"Jarque-Bera p-value: {jb_p:.4f}\")\n",
    "print(f\"Shapiro-Wilk p-value: {sw_p:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breusch–Pagan test for heteroskedasticity. H0: constant variance (homoscedastic). Small p-value suggests heteroskedasticity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_const = sm.add_constant(X_test)\n",
    "ols = sm.OLS(y_test, X_test_const).fit()\n",
    "bp_test = het_breuschpagan(ols.resid, X_test_const)\n",
    "labels = ['LM stat', 'LM p-value', 'F stat', 'F p-value']\n",
    "pd.Series(bp_test, index=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Multicollinearity via VIF\n",
    "Compute Variance Inflation Factors (VIF). VIF > 5–10 can indicate problematic multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vif = sm.add_constant(X_train)\n",
    "vif_df = pd.DataFrame({\n",
    "    'feature': X_train_vif.columns,\n",
    "    'VIF': [variance_inflation_factor(X_train_vif.values, i) for i in range(X_train_vif.shape[1])]\n",
    "})\n",
    "vif_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an intentionally collinear feature and re-evaluate VIF to see it spike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_collinear = X_train.copy()\n",
    "X_train_collinear['x_dup'] = X_train_collinear['x1'] * 1.0 + np.random.normal(0, 0.01, size=len(X_train_collinear))\n",
    "X_train_collinear_const = sm.add_constant(X_train_collinear)\n",
    "vif_collinear = pd.DataFrame({\n",
    "    'feature': X_train_collinear_const.columns,\n",
    "    'VIF': [variance_inflation_factor(X_train_collinear_const.values, i) for i in range(X_train_collinear_const.shape[1])]\n",
    "})\n",
    "vif_collinear.sort_values('VIF', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Optional: Statsmodels OLS summary\n",
    "Provides coefficients, standard errors, and more detailed diagnostics. Useful for interpretation when assumptions are (approximately) satisfied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "optional"
    ]
   },
   "outputs": [],
   "source": [
    "X_train_const = sm.add_constant(X_train)\n",
    "ols_train = sm.OLS(y_train, X_train_const).fit()\n",
    "ols_train.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Handling nonlinearity and scaling (brief)\n",
    "Use polynomial features to capture curvature. Note: increases risk of multicollinearity — use scaling and possibly regularization in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_pipe = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('linreg', LinearRegression())\n",
    "])\n",
    "poly_pipe.fit(X_train, y_train)\n",
    "y_pred_poly = poly_pipe.predict(X_test)\n",
    "r2_poly = r2_score(y_test, y_pred_poly)\n",
    "rmse_poly = mean_squared_error(y_test, y_pred_poly, squared=False)\n",
    "print({'R2_linear': r2, 'RMSE_linear': rmse, 'R2_poly': r2_poly, 'RMSE_poly': rmse_poly})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "Complete the tasks below. Instructor solution cells are hidden; expand them if needed.\n",
    "\n",
    "1. Outliers: Inject a few extreme outliers into the training target and re-fit the model. How do metrics and residual plots change? Try trimming or using a robust regression (HuberRegressor) and compare.\n",
    "2. California Housing: Load `fetch_california_housing()` and repeat the workflow (train/test split, fit, diagnostics). Compare assumptions and performance vs. synthetic data.\n",
    "3. Feature Engineering: Try adding interaction terms or log-transforming skewed features (on California data). Re-check VIF, residuals, and metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "outputs": [],
   "source": [
    "# Exercise 1: Outliers\n",
    "# TODO: Copy X_train, y_train to new variables, inject outliers into y, fit LinearRegression, and compare metrics/plots.\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Solution 1 (hidden)\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "\n",
    "X_train_o = X_train.copy()\n",
    "y_train_o = y_train.copy()\n",
    "idx = np.random.choice(len(y_train_o), size=10, replace=False)\n",
    "y_train_o.iloc[idx] += np.random.normal(300, 50, size=len(idx))\n",
    "\n",
    "lr_o = LinearRegression().fit(X_train_o, y_train_o)\n",
    "y_pred_o = lr_o.predict(X_test)\n",
    "print('Linear with outliers:', r2_score(y_test, y_pred_o), mean_squared_error(y_test, y_pred_o, squared=False))\n",
    "\n",
    "hub = HuberRegressor().fit(X_train_o, y_train_o)\n",
    "y_pred_h = hub.predict(X_test)\n",
    "print('Huber (robust):', r2_score(y_test, y_pred_h), mean_squared_error(y_test, y_pred_h, squared=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "outputs": [],
   "source": [
    "# Exercise 2: California Housing\n",
    "# TODO: Load dataset, fit LinearRegression, run diagnostics (residual plot, QQ, BP test, VIF), compare to synthetic.\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Solution 2 (hidden)\n",
    "cal = fetch_california_housing()\n",
    "Xc = pd.DataFrame(cal.data, columns=cal.feature_names)\n",
    "yc = pd.Series(cal.target, name='MedHouseVal')\n",
    "Xc_train, Xc_test, yc_train, yc_test = train_test_split(Xc, yc, test_size=0.2, random_state=42)\n",
    "lr_c = LinearRegression().fit(Xc_train, yc_train)\n",
    "yc_pred = lr_c.predict(Xc_test)\n",
    "print('California R2, RMSE:', r2_score(yc_test, yc_pred), mean_squared_error(yc_test, yc_pred, squared=False))\n",
    "\n",
    "resid_c = yc_test - yc_pred\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12,4))\n",
    "axes[0].scatter(yc_pred, resid_c, alpha=0.5)\n",
    "axes[0].axhline(0, color='red', ls='--')\n",
    "axes[0].set_title('Residuals vs Predicted (CA)')\n",
    "sm.ProbPlot(resid_c, fit=True).qqplot(line='45', ax=axes[1])\n",
    "axes[1].set_title('QQ Plot (CA)')\n",
    "plt.show()\n",
    "\n",
    "Xc_test_const = sm.add_constant(Xc_test)\n",
    "bp = het_breuschpagan(resid_c, Xc_test_const)\n",
    "print('BP p-value:', bp[1], bp[3])\n",
    "\n",
    "Xc_train_const = sm.add_constant(Xc_train)\n",
    "vif_ca = pd.DataFrame({\n",
    "    'feature': Xc_train_const.columns,\n",
    "    'VIF': [variance_inflation_factor(Xc_train_const.values, i) for i in range(Xc_train_const.shape[1])]\n",
    "})\n",
    "vif_ca.sort_values('VIF', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "outputs": [],
   "source": [
    "# Exercise 3: Feature Engineering\n",
    "# TODO: Try interactions or log-transform certain features (e.g., log(1 + MedInc)) and compare metrics/assumptions.\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Solution 3 (hidden)\n",
    "Xc2 = Xc.copy()\n",
    "if 'MedInc' in Xc2.columns:\n",
    "    Xc2['log_MedInc'] = np.log1p(Xc2['MedInc'])\n",
    "if set(['AveRooms','AveBedrms']).issubset(Xc2.columns):\n",
    "    Xc2['Rooms_per_Bedroom'] = Xc2['AveRooms'] / (Xc2['AveBedrms'] + 1e-6)\n",
    "\n",
    "Xc2_train, Xc2_test, yc_train, yc_test = train_test_split(Xc2, yc, test_size=0.2, random_state=42)\n",
    "lr2 = LinearRegression().fit(Xc2_train, yc_train)\n",
    "yc2_pred = lr2.predict(Xc2_test)\n",
    "print('Engineered R2, RMSE:', r2_score(yc_test, yc2_pred), mean_squared_error(yc_test, yc2_pred, squared=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-up\n",
    "Checklist for linear regression practice:\n",
    "- [ ] Split data properly and avoid leakage\n",
    "- [ ] Fit baseline model and record metrics\n",
    "- [ ] Plot residuals vs. predicted; look for patterns\n",
    "- [ ] Check residual normality (hist/QQ), run normality tests if needed\n",
    "- [ ] Test homoscedasticity (e.g., Breusch–Pagan)\n",
    "- [ ] Compute VIF for multicollinearity\n",
    "- [ ] Revisit outliers, transformations, and features as needed\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
