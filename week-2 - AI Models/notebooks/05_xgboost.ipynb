{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 â€” XGBoost (Hands-on)\n",
    "\n",
    "Objectives:\n",
    "- Train an XGBoost classifier on tabular data with early stopping\n",
    "- Understand core hyperparameters: learning rate (eta), max_depth, subsample, colsample_bytree\n",
    "- Evaluate with ROC-AUC, classification report; compare validation vs test\n",
    "- Inspect feature importance and discuss leakage/overfitting risks\n",
    "\n",
    "Assumptions:\n",
    "- Boosting: models improve by fitting residuals/gradients of prior iterations\n",
    "- Best for structured/tabular data; handles numeric features well\n",
    "\n",
    "Cautions/Data Prep:\n",
    "- Missing values are handled natively but can still impact results\n",
    "- Sensitive to overfitting: use early stopping, regularization, and CV\n",
    "- Watch for data leakage (ensure proper splits; avoid using future info)\n",
    "- Tuning (eta, max_depth, subsample, colsample_*) is crucial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid', context='notebook')\n",
    "np.random.seed(42)\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score, RocCurveDisplay, confusion_matrix\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Load dataset and create train/validation/test split\n",
    "We will create a validation set for early stopping separate from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target, name='target')\n",
    "\n",
    "# Train/test split first\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "# Split validation from the training portion\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.2, random_state=42, stratify=y_train_full\n",
    ")\n",
    "X.shape, y.value_counts(normalize=True).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Baseline XGBoost with early stopping\n",
    "Keep CPU-friendly settings. Early stopping halts when validation metric doesn't improve for `early_stopping_rounds` rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='auc',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    tree_method='hist'  # fast CPU histogram algorithm\n",
    ")\n",
    "\n",
    "xgb.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "    verbose=False,\n",
    "    early_stopping_rounds=20\n",
    ")\n",
    "\n",
    "best_it = int(xgb.best_iteration) if hasattr(xgb, 'best_iteration') else None\n",
    "best_score = float(xgb.best_score) if hasattr(xgb, 'best_score') else None\n",
    "print({'best_iteration': best_it, 'best_val_auc': round(best_score, 4) if best_score else None})\n",
    "\n",
    "proba_val = xgb.predict_proba(X_val)[:,1]\n",
    "proba_test = xgb.predict_proba(X_test)[:,1]\n",
    "print({'val_auc': round(roc_auc_score(y_val, proba_val), 4),\n",
    "       'test_auc': round(roc_auc_score(y_test, proba_test), 4)})\n",
    "pred_test = (proba_test >= 0.5).astype(int)\n",
    "print(classification_report(y_test, pred_test, digits=3))\n",
    "confusion_matrix(y_test, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RocCurveDisplay.from_predictions(y_test, proba_test, name='XGBoost (test)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Feature importance\n",
    "XGBoost provides multiple importance types. Here we show gain-based importance via `feature_importances_` (by default: weight/gain depending on version), and also the built-in plot for convenience (optional if GUI backends are limited)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = pd.Series(xgb.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "imp.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Hyperparameter sweeps (CPU-friendly)\n",
    "Small grids for `max_depth` and `learning_rate`. Observe validation and test AUC trends. In practice, use cross-validation for robust selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval(eta=0.05, depth=4, subs=0.8, cols=0.8):\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=400,\n",
    "        learning_rate=eta,\n",
    "        max_depth=depth,\n",
    "        subsample=subs,\n",
    "        colsample_bytree=cols,\n",
    "        reg_lambda=1.0,\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='auc',\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        tree_method='hist'\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=False,\n",
    "        early_stopping_rounds=15\n",
    "    )\n",
    "    vauc = roc_auc_score(y_val, model.predict_proba(X_val)[:,1])\n",
    "    tauc = roc_auc_score(y_test, model.predict_proba(X_test)[:,1])\n",
    "    return vauc, tauc\n",
    "\n",
    "depths = [3, 4, 5, 6]\n",
    "etas = [0.03, 0.05, 0.1]\n",
    "results = []\n",
    "for d in depths:\n",
    "    for e in etas:\n",
    "        v,t = train_eval(eta=e, depth=d)\n",
    "        results.append((d, e, v, t))\n",
    "\n",
    "res_df = pd.DataFrame(results, columns=['max_depth','eta','val_auc','test_auc'])\n",
    "display(res_df.sort_values(['val_auc','test_auc'], ascending=False).head(10))\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "for d in depths:\n",
    "    sub = res_df[res_df.max_depth==d]\n",
    "    plt.plot(sub.eta, sub.val_auc, marker='o', label=f'depth={d}')\n",
    "plt.xlabel('learning_rate (eta)')\n",
    "plt.ylabel('Validation AUC')\n",
    "plt.title('Validation AUC vs eta by max_depth')\n",
    "plt.legend(); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsampling often improves generalization with some variance. Try a quick sweep for `subsample` and `colsample_bytree` at a fixed depth/eta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = [0.6, 0.8, 1.0]\n",
    "cols = [0.6, 0.8, 1.0]\n",
    "grid = []\n",
    "for s in subs:\n",
    "    for c in cols:\n",
    "        v,t = train_eval(eta=0.05, depth=4, subs=s, cols=c)\n",
    "        grid.append((s, c, v, t))\n",
    "grid_df = pd.DataFrame(grid, columns=['subsample','colsample_bytree','val_auc','test_auc'])\n",
    "display(grid_df.sort_values(['val_auc','test_auc'], ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Overfitting and leakage notes\n",
    "- Monitor the gap between validation and test AUC; large gaps may indicate overfitting or unlucky splits\n",
    "- Ensure all preprocessing (if any) is fit only on training data, then applied to val/test\n",
    "- Beware engineered features that accidentally use label information (e.g., target means computed across full data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "Instructor solution cells are hidden/collapsed.\n",
    "1. Learning rate sweep: Try `eta` in [0.01, 0.03, 0.05, 0.1] at `max_depth=4` and plot validation vs test AUC lines.\n",
    "2. Early stopping sensitivity: Compare `early_stopping_rounds` = 5 vs 20 vs 50; report best_iteration and test AUC.\n",
    "3. Sanity check: Shuffle labels on the training set and show that validation AUC drops near 0.5 (random). Restore labels afterwards.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "tags": ["exercise"] },
   "outputs": [],
   "source": [
    "# Exercise 1: Learning rate sweep\n",
    "# TODO: Evaluate eta in [0.01, 0.03, 0.05, 0.1] at depth=4 with early stopping; plot val/test AUC curves.\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "tags": ["solution"], "jupyter": { "source_hidden": true } },
   "outputs": [],
   "source": [
    "# Solution 1 (hidden)\n",
    "etas2 = [0.01, 0.03, 0.05, 0.1]\n",
    "vals, tests = [], []\n",
    "for e in etas2:\n",
    "    v,t = train_eval(eta=e, depth=4)\n",
    "    vals.append(v); tests.append(t)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(etas2, vals, marker='o', label='Val AUC')\n",
    "plt.plot(etas2, tests, marker='s', label='Test AUC')\n",
    "plt.xlabel('eta'); plt.ylabel('AUC'); plt.title('AUC vs learning rate (depth=4)')\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n",
    "list(zip(etas2, [round(v,3) for v in vals], [round(t,3) for t in tests]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "tags": ["exercise"] },
   "outputs": [],
   "source": [
    "# Exercise 2: Early stopping sensitivity\n",
    "# TODO: Train with early_stopping_rounds in [5, 20, 50] and record best_iteration and test AUC.\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "tags": ["solution"], "jupyter": { "source_hidden": true } },
   "outputs": [],
   "source": [
    "# Solution 2 (hidden)\n",
    "rows = []\n",
    "for es in [5, 20, 50]:\n",
    "    m = XGBClassifier(n_estimators=600, learning_rate=0.05, max_depth=4, subsample=0.8, colsample_bytree=0.8,\n",
    "                      objective='binary:logistic', eval_metric='auc', n_jobs=-1, random_state=42, tree_method='hist')\n",
    "    m.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=es, verbose=False)\n",
    "    it = getattr(m, 'best_iteration', None)\n",
    "    auc_t = roc_auc_score(y_test, m.predict_proba(X_test)[:,1])\n",
    "    rows.append((es, it, auc_t))\n",
    "pd.DataFrame(rows, columns=['early_stopping_rounds','best_iteration','test_auc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "tags": ["exercise"] },
   "outputs": [],
   "source": [
    "# Exercise 3: Sanity check with label shuffle\n",
    "# TODO: Shuffle y_train (keeping X_train) and refit a small model; show val AUC ~ 0.5. Then restore original labels.\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": { "tags": ["solution"], "jupyter": { "source_hidden": true } },
   "outputs": [],
   "source": [
    "# Solution 3 (hidden)\n",
    "y_train_shuff = y_train.sample(frac=1.0, random_state=123).reset_index(drop=True)\n",
    "X_train_shuff = X_train.reset_index(drop=True)\n",
    "m = XGBClassifier(n_estimators=200, learning_rate=0.1, max_depth=3, subsample=0.8, colsample_bytree=0.8,\n",
    "                  objective='binary:logistic', eval_metric='auc', n_jobs=-1, random_state=123, tree_method='hist')\n",
    "m.fit(X_train_shuff, y_train_shuff, eval_set=[(X_val, y_val)], early_stopping_rounds=10, verbose=False)\n",
    "val_auc = roc_auc_score(y_val, m.predict_proba(X_val)[:,1])\n",
    "round(val_auc,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-up checklist\n",
    "- [ ] Create train/val/test splits; avoid leakage\n",
    "- [ ] Use early stopping and track best_iteration\n",
    "- [ ] Tune eta, max_depth, subsample, colsample_bytree\n",
    "- [ ] Compare validation vs test AUC to gauge generalization\n",
    "- [ ] Inspect feature importance carefully; validate with robust metrics\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
